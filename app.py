# app.py
import os
import io
import streamlit as st
import joblib
import jieba
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---- transformers ç”¨äº BERT ----
import torch
from transformers import BertTokenizer, BertForSequenceClassification


# ---- è®¾ç½®ä¸­æ–‡å­—ä½“ ----
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']   # æ˜¾ç¤ºä¸­æ–‡
matplotlib.rcParams['axes.unicode_minus'] = False      # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ---- å®šä¹‰ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„åˆ†è¯å‡½æ•° ----
def my_tokenizer(text):
    # TF-IDF æ¨¡å‹ä¸­çš„ tokenizer å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œå¦åˆ™æ— æ³•åŠ è½½
    return text.split()
# =====================================================
#                 ç¼“å­˜åŠ è½½ TF-IDF æ¨¡å‹
# =====================================================
@st.cache_resource
def load_tfidf_model():
    vect = joblib.load("model/tfidf_vectorizer.joblib")
    clf = joblib.load("model/logistic.joblib")
    return vect, clf


# =====================================================
#                 ç¼“å­˜åŠ è½½ BERT æ¨¡å‹
# =====================================================
@st.cache_resource
def load_bert_model():
    if not os.path.exists("model/bert_sentiment"):
        return None, None
    tokenizer = BertTokenizer.from_pretrained("model/bert_sentiment")
    model = BertForSequenceClassification.from_pretrained("model/bert_sentiment")
    model.eval()
    return tokenizer, model


# =====================================================
#                 æ–‡æœ¬é¢„å¤„ç†
# =====================================================
def preprocess_text(text):
    return " ".join(jieba.lcut(text))


# =====================================================
#                 Streamlit é¡µé¢é…ç½®
# =====================================================
st.set_page_config(page_title="ä¸­æ–‡æƒ…æ„Ÿåˆ†æç³»ç»Ÿ", page_icon="ğŸ’¬", layout="centered")
st.title("ğŸ’¬ ä¸­æ–‡æƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
st.markdown("è¯·è¾“å…¥ä¸€å¥è¯ï¼Œæˆ–ä¸Šä¼  CSV æ–‡ä»¶è¿›è¡Œæ‰¹é‡æƒ…æ„Ÿåˆ†æã€‚")


# =====================================================
#             ä¾§è¾¹æ ï¼šæ¨¡å‹é€‰æ‹©
# =====================================================
st.sidebar.header("âš™ï¸ æ¨¡å‹è®¾ç½®")

model_choice = st.sidebar.radio(
    "é€‰æ‹©æ¨¡å‹ï¼š",
    ("TF-IDF + LRï¼ˆè½»é‡æ¨¡å‹ï¼‰", "BERT ä¸­æ–‡æ¨¡å‹ï¼ˆé«˜ç²¾åº¦ï¼‰")
)

use_bert = model_choice.startswith("BERT")

# åŠ è½½æ¨¡å‹ï¼ˆæ ¹æ®é€‰æ‹©ï¼‰
vect, clf = load_tfidf_model()
tokenizer, bert_model = load_bert_model()


# =====================================================
#             é¡µé¢é€‰é¡¹å¡
# =====================================================
tab1, tab2, tab3 = st.tabs(["ğŸ”¹ å•å¥åˆ†æ", "ğŸ“‚ æ‰¹é‡ä¸Šä¼ åˆ†æ", "ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”"])


# =====================================================
# ğŸ”¹ å•å¥åˆ†æ
# =====================================================
with tab1:
    user_input = st.text_area("è¯·è¾“å…¥æ–‡æœ¬ï¼š", height=120, placeholder="ä¾‹å¦‚ï¼šè¿™å®¶é…’åº—éå¸¸å¹²å‡€ï¼ŒæœåŠ¡æ€åº¦å¾ˆå¥½ã€‚")

    if st.button("åˆ†ææƒ…æ„Ÿ", key="single"):
        if not user_input.strip():
            st.warning("è¯·è¾“å…¥ä¸€å¥è¯å†è¿›è¡Œåˆ†æï½")
        else:
            # ================== TF-IDF æ¨¡å‹ ==================
            if not use_bert:
                cut_text = preprocess_text(user_input)
                X = vect.transform([cut_text])
                pred = clf.predict(X)[0]
                prob = clf.predict_proba(X)[0]

                sentiment = "æ­£é¢ ğŸ˜„" if pred == 1 else "è´Ÿé¢ ğŸ˜ "
                confidence = prob[pred]

                st.subheader(f"åˆ†æç»“æœï¼š{sentiment}")
                st.write(f"ç½®ä¿¡åº¦ï¼š**{confidence:.2f}**")

                # ---- å¯è§†åŒ–ç½®ä¿¡åº¦ ----
                labels = ["è´Ÿé¢", "æ­£é¢"]
                fig, ax = plt.subplots()
                ax.bar(labels, prob, color=["red", "green"])
                ax.set_ylim([0, 1])
                ax.set_ylabel("æ¦‚ç‡")
                ax.set_title("æƒ…æ„Ÿç½®ä¿¡åº¦åˆ†å¸ƒ")
                st.pyplot(fig)

            # ================== BERT æ¨¡å‹ ==================
            else:
                if tokenizer is None:
                    st.error("âŒ æœªæ£€æµ‹åˆ° BERT æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œ bert_finetune.py è¿›è¡Œè®­ç»ƒã€‚")
                else:
                    inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding=True)
                    with torch.no_grad():
                        outputs = bert_model(**inputs)
                        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
                        pred = torch.argmax(probs, dim=1).item()
                        confidence = probs[0][pred].item()

                    sentiment = "æ­£é¢ ğŸ˜„" if pred == 1 else "è´Ÿé¢ ğŸ˜ "

                    st.subheader(f"BERT åˆ†æç»“æœï¼š{sentiment}")
                    st.write(f"ç½®ä¿¡åº¦ï¼š**{confidence:.2f}**")

                    # å¯è§†åŒ–
                    labels = ["è´Ÿé¢", "æ­£é¢"]
                    fig, ax = plt.subplots()
                    ax.bar(labels, probs.numpy()[0], color=["red", "green"])
                    ax.set_ylim([0, 1])
                    ax.set_ylabel("æ¦‚ç‡")
                    ax.set_title("BERT æƒ…æ„Ÿç½®ä¿¡åº¦åˆ†å¸ƒ")
                    st.pyplot(fig)


# =====================================================
# ğŸ“‚ æ‰¹é‡ä¸Šä¼ åˆ†æ
# =====================================================
with tab2:
    st.write("ä¸Šä¼ ä¸€ä¸ª CSV æ–‡ä»¶ï¼ˆéœ€åŒ…å«åä¸º `text` çš„åˆ—ï¼‰ã€‚")

    uploaded_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=["csv"])

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)

            if "text" not in df.columns:
                st.error("CSV æ–‡ä»¶å¿…é¡»åŒ…å«åˆ—å `text`ã€‚")
            else:
                st.success(f"æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•ã€‚")

                # ================== TF-IDF æ‰¹é‡åˆ†æ ==================
                if not use_bert:
                    df["text_cut"] = df["text"].astype(str).apply(preprocess_text)
                    X = vect.transform(df["text_cut"])
                    preds = clf.predict(X)
                    probs = clf.predict_proba(X)

                    df["pred_label"] = preds
                    df["pred_sentiment"] = df["pred_label"].map({1: "æ­£é¢", 0: "è´Ÿé¢"})
                    df["confidence"] = [max(p) for p in probs]

                # ================== BERT æ‰¹é‡åˆ†æ ==================
                else:
                    if tokenizer is None:
                        st.error("âŒ æœªæ£€æµ‹åˆ° BERT æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒã€‚")
                    else:
                        preds_list, conf_list = [], []
                        for text in df["text"].astype(str).tolist():
                            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
                            with torch.no_grad():
                                outputs = bert_model(**inputs)
                                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
                                pred = torch.argmax(probs, dim=1).item()

                            preds_list.append(pred)
                            conf_list.append(probs[0][pred].item())

                        df["pred_label"] = preds_list
                        df["pred_sentiment"] = df["pred_label"].map({1: "æ­£é¢", 0: "è´Ÿé¢"})
                        df["confidence"] = conf_list

                # ==== æ˜¾ç¤ºç»“æœ ====
                st.subheader("ğŸ“Š åˆ†æç»“æœé¢„è§ˆ")
                st.dataframe(df[["text", "pred_sentiment", "confidence"]].head(10))

                # ==== å…³é”®è¯äº‘ï¼ˆä»…æ­£/è´ŸåŒºåˆ†ï¼‰ ====
                from wordcloud import WordCloud
                st.subheader("â˜ï¸ æƒ…æ„Ÿå…³é”®è¯äº‘")

                font_path = "C:\\Windows\\Fonts\\SimHei.ttf"
                if not os.path.exists(font_path):
                    st.warning("âš ï¸ æœªæ‰¾åˆ° SimHei.ttf å­—ä½“ï¼Œä¸­æ–‡å¯èƒ½ä¼šä¹±ç ã€‚")

                df["text_cut"] = df["text"].astype(str).apply(preprocess_text)

                # æ­£é¢è¯äº‘
                pos_text = " ".join(df[df["pred_label"] == 1]["text_cut"])
                if pos_text.strip():
                    st.markdown("### ğŸ˜Š æ­£é¢è¯„è®ºå…³é”®è¯äº‘")
                    wc = WordCloud(font_path=font_path, background_color="white",
                                   width=600, height=400, colormap="Greens").generate(pos_text)
                    fig, ax = plt.subplots()
                    ax.imshow(wc, interpolation="bilinear")
                    ax.axis("off")
                    st.pyplot(fig)

                # è´Ÿé¢è¯äº‘
                neg_text = " ".join(df[df["pred_label"] == 0]["text_cut"])
                if neg_text.strip():
                    st.markdown("### ğŸ˜  è´Ÿé¢è¯„è®ºå…³é”®è¯äº‘")
                    wc = WordCloud(font_path=font_path, background_color="white",
                                   width=600, height=400, colormap="Reds").generate(neg_text)
                    fig, ax = plt.subplots()
                    ax.imshow(wc, interpolation="bilinear")
                    ax.axis("off")
                    st.pyplot(fig)

                # ==== CSV ä¸‹è½½ ====
                csv_buf = io.BytesIO()
                df.to_csv(csv_buf, index=False, encoding="utf-8-sig")
                csv_buf.seek(0)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½åˆ†æç»“æœ CSV",
                    data=csv_buf,
                    file_name="sentiment_result.csv",
                    mime="text/csv"
                )

        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}")

import json

with tab3:
    st.header("ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”")

    # è¯»å–æ€§èƒ½æ–‡ä»¶
    try:
        with open("model/performance.json", "r", encoding="utf-8") as f:
            perf = json.load(f)
    except:
        st.error("âš ï¸ æœªæ‰¾åˆ° model/performance.jsonï¼Œæ— æ³•æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”ã€‚")
        st.stop()

    tfidf = perf["tfidf"]
    bert = perf["bert"]

    # æ•°å€¼è¡¨æ ¼
    st.subheader("ğŸ” æ¨¡å‹æŒ‡æ ‡å¯¹æ¯”è¡¨")
    df_perf = pd.DataFrame({
        "æŒ‡æ ‡": ["å‡†ç¡®ç‡ (Accuracy)", "ç²¾ç¡®ç‡ (Precision)", "å¬å›ç‡ (Recall)", "F1-score"],
        "TF-IDF + LR": [tfidf["accuracy"], tfidf["precision"], tfidf["recall"], tfidf["f1"]],
        "BERT": [bert["accuracy"], bert["precision"], bert["recall"], bert["f1"]]
    })

    st.dataframe(df_perf)

    # å¯è§†åŒ–å¯¹æ¯”ï¼ˆé›·è¾¾å›¾ï¼‰
    st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾")
    labels = ["å‡†ç¡®ç‡", "ç²¾ç¡®ç‡", "å¬å›ç‡", "F1-score"]
    tfidf_values = [tfidf["accuracy"], tfidf["precision"], tfidf["recall"], tfidf["f1"]]
    bert_values = [bert["accuracy"], bert["precision"], bert["recall"], bert["f1"]]

    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False)
    tfidf_values += tfidf_values[:1]
    bert_values += bert_values[:1]
    angles = np.concatenate((angles, [angles[0]]))

    fig, ax = plt.subplots(subplot_kw={"projection": "polar"})
    ax.plot(angles, tfidf_values, "o-", label="TF-IDF + LR")
    ax.fill(angles, tfidf_values, alpha=0.25)

    ax.plot(angles, bert_values, "o-", label="BERT")
    ax.fill(angles, bert_values, alpha=0.25)

    ax.set_thetagrids(angles[:-1] * 180 / np.pi, labels)
    ax.set_title("æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾")
    ax.legend(loc="best")
    st.pyplot(fig)
